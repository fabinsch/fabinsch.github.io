---
title: "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning"
collection: publications
authors: 'Franki Nguimatsia Tiofack*, Th√©otime Le Hellard*, <b>Fabian Schramm*</b>, Nicolas Perrin-Gilbert, Justin Carpentier <br><span style="font-size:0.9em; font-style:italic;">* Equal contribution</span>'
permalink: /publication/2025-12-03-gfp
excerpt: 'We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor to focus on learning from high-value actions in offline reinforcement learning.'
date: 2026-05-01
venue: 'Fourteenth International Conference on Learning Representations (ICLR)'
paperurl: 'https://arxiv.org/abs/2512.03973'
citation: 'Tiofack, F. N., Le Hellard, T., Schramm, F., Perrin-Gilbert, N., & Carpentier, J. (2026). &quot;Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning.&quot; <i>ICLR 2026</i>.'
doi: '10.48550/arXiv.2512.03973'
---
Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks.

[Download paper here](https://arxiv.org/abs/2512.03973)